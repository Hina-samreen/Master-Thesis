{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install transformers\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test_subset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text set for getting the results from LLM\n",
    "texts = list(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat propmt used to get the labels of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"google/gemma-1.1-7b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "prompt = f\"Classify the sentiment of each text into ONE of the three classes: neutral, negative or positive. Split the answer in a dictionary with two keys:Text and Label. Text: \"\n",
    "\n",
    "answers = []\n",
    "#text_list =[ \"Ukraine deckt auf üòÇüòÇüòÇüòÇüòÇ\\nDie mussten was sagen, nicht das man denkt der Staat w√§re korrupt und es w√ºrde keinen Kampf dagegen geben ‚òùÔ∏è\\n\\nKorrupt bleibt korrupt - auch die, da wird sich auch nichts √§nder üòÜ\",'Nur Politiker wollen Krieg ! Was wenn keiner mehr Bock hat f√ºr diese 5 Politiker zu sterben warum drehen wir den Spie√ü nich einfach um ? üòï frage f√ºr n Freund']\n",
    "for text in texts:\n",
    "    chat_template = [{\"role\" : \"user\", \"content\" : f\"{prompt} {text}\"}]\n",
    "\n",
    "    chat_prompt = tokenizer.apply_chat_template(chat_template, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer.encode(chat_prompt, add_special_tokens=False, return_tensors=\"pt\", max_length=512)\n",
    "    outputs = model.generate(input_ids=inputs.to(\"cuda\"), max_new_tokens = 512)\n",
    "    answer = tokenizer.decode(outputs[0])\n",
    "    answers.append(answer)\n",
    "    print(answer)\n",
    "print(len(answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the results in the desired format , so as to evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_after = []\n",
    "labels_after = []\n",
    "for answer in answers:\n",
    "    if len((answer.split('<start_of_turn>model')[1].split(':'))) > 2:\n",
    "        texts_after.append(answer.split('<start_of_turn>model')[1].split(':')[1].split('\"Label\"')[0].strip(' \"\" '))\n",
    "        labels_after.append(answer.split('<start_of_turn>model')[1].split(':')[2].split('\\n')[0].strip(' \"\" '))\n",
    "    else:\n",
    "        print('not proper',answer.split('<start_of_turn>model')[1].split(':'))\n",
    "    \n",
    "print(len(texts_after))\n",
    "print(len(labels_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the row which was not preoperly formatted as seen from above results\n",
    "df = df[(df['text'] != 'Was ist nur in Deutschland los, dass man ein furchtbares Unrecht nicht benennen darf ohne als Antisemit angeprangert zu werden? ‚ÄûNie wieder‚Äú‚Ä¶ hatten wir das nach dem 2. Weltkrieg nicht verk√ºndet und seitdem in Sonntagsreden endlos wiederholt? Wie kann unsere Politik, k√∂nnen die Medien die Gr√§ueltaten Israels rechtfertigen? Fragt man sich zudem nicht, wieso in Harvard und auch sonst in gro√üen Teilen der Welt ein Ende der Bombardierung gefordert wird? Ach, ich frage mich √ºbrigens, ob es um die √ñl- und Gasvorkommen in Gaza geht üôå')]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['predicted_sentiment_llm'] = labels_after\n",
    "#getting only the classes 'Negative','Positive,'and 'Neutral' and ignoring other classes\n",
    "df = df[('Negative' == df['predicted_sentiment_llm']) | ('Positive' == df['predicted_sentiment_llm']) | ('Neutral' == df['predicted_sentiment_llm'])]\n",
    "df = df.reset_index(drop=True)\n",
    "#lower casing the class names\n",
    "for i in range(len(df)):\n",
    "    df.loc[i,'predicted_sentiment_llm'] = str(df['predicted_sentiment_llm'][i]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct predictions: 196\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "for i in range(len(df)):\n",
    "    if df['sentiment'][i] == df['predicted_sentiment_llm'][i]:\n",
    "        correct_predictions += 1\n",
    "print('correct predictions:',correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of google gemma before fine tuning is 76.26459143968872 %\n"
     ]
    }
   ],
   "source": [
    "Accuracy = (correct_predictions/len(df))*100\n",
    "print('Accuracy of google gemma before fine tuning is',Accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ResultsBeforeFineTuning_SequnceClassification_gemmaGoogle.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_saint_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
