{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install transformers\n",
    "%pip install accelerate\n",
    "%pip install xformers\n",
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer the github repository 'https://github.com/unslothai/unsloth' for more information about fine-tuning with unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "%pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ltstorage/home/1samreen/miniconda3/envs/new_saint_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.138 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
    "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
    "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\" ,        # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-7b-it\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data creation and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>negative object_spans</th>\n",
       "      <th>negative spans</th>\n",
       "      <th>positive spans</th>\n",
       "      <th>positive object_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>1237</td>\n",
       "      <td>die pandemie mag zu ende sein. viele geimpfte stecken weiter in ihren impfsch√§den. üôÅ</td>\n",
       "      <td>negative</td>\n",
       "      <td>die pandemie</td>\n",
       "      <td>['viele geimpfte stecken weiter in ihren impfsch√§den', 'üôÅ']</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>210</td>\n",
       "      <td>Wir sind hier nicht bei 'W√ºnsch dir was', sondern bei 'So isses' und das gilt auch f√ºr den schlechtesten Bundesminister f√ºr #Wirtschaft und #Klimaschutz Robert #Habeck \\nKeinen Bock mehr...was ist denn das f√ºr eine Aussage?</td>\n",
       "      <td>negative</td>\n",
       "      <td>['Klimaschutz', 'Habeck ']</td>\n",
       "      <td>['schlechtesten Bundesminister', 'Keinen Bock mehr']</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>494</td>\n",
       "      <td>Es kann wirklich niemand mehr behaupten, dass man nicht wusste, was man w√§hlt!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>123</td>\n",
       "      <td>Da ist der falsche bestraft wurden</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>der falsche bestraft wurden</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>1470</td>\n",
       "      <td>Vor 500 Jahren h√§tte man gesagt, die #AfD sei des Teufels Wiederg√§nger. Man h√§tte sie geteert und gefedert, auf einen Holzpfahl gespie√üt, mit Steinen beschwert und f√ºnf Meter tief vergraben. Nun leben wir in einer Demokratie und m√ºssen sie ertragen. Aber #LautgegenRechts d√ºrfen wir sein!</td>\n",
       "      <td>negative</td>\n",
       "      <td>#AfD</td>\n",
       "      <td>['Man h√§tte sie geteert und gefedert, auf einen Holzpfahl gespie√üt, mit Steinen beschwert und f√ºnf Meter tief vergraben.', 'Nun leben wir in einer Demokratie und m√ºssen sie ertragen']</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>1443</td>\n",
       "      <td>Zwei wahre S√§tze f√ºr den Abend:\\nJedes einzelne zivile Opfer im Gazastreifen geht auf das Konto der Hamas. Keine relevante Armee der Welt handelt nach h√∂heren moralischen Ma√üst√§ben als die IDF.</td>\n",
       "      <td>negative</td>\n",
       "      <td>Hamas</td>\n",
       "      <td>zivile Opfer im Gazastreifen</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>4</td>\n",
       "      <td>Danke S√ºdafrikaüëè wenigstens schaut ihr nicht weg oder seid keine Marionette der USA!</td>\n",
       "      <td>positive</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Danke S√ºdafrikaüëè', 'seid keine Marionette']</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>645</td>\n",
       "      <td>Ohne angeben zu wollen, habe ich #Putin schon vor 15 Jahren absolut alles zugetraut. Der Typ war schon immer total skrupellos &amp; hinterliess nur Schutt &amp; Asche. Nun l√§uft ihm die Zeit davon, seine Pl√§ne wie gewohnt umzusetzen und die #Demokratie r√ºckt n√§her an #Russland heran. Der offene √úberfall auf die #Ukraine ist klar ein Zeichen daf√ºr. Er wird immer brutaler &amp; unberechenbarer werden. Wir sollten uns darauf einstellen &amp; ihn stoppen, bevor der ganze Kontinent hineingezogen wird.\\n\\n#NATO #SupportUkraine #Baltikum #Estland #Lettland #Litauen #Georgien #Polen #Moldawien #Finnland\\n@Bundeskanzler\\n \\n@fdp\\n \\n@ABaerbock\\n \\n@ignaziocassis\\n \\n@Die_Gruenen\\n \\n@spdde\\n \\n@spdbt</td>\n",
       "      <td>negative</td>\n",
       "      <td>#Putin</td>\n",
       "      <td>['Der Typ war schon immer total skrupellos', 'hinterliess nur Schutt &amp;amp; Asche.', 'brutaler &amp;amp; unberechenbarer']</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>1142</td>\n",
       "      <td>Bitte Deutschland noch mehr Waffen liefern, bis alle Menschen da get√∂tet werden . Dann kann die Welt friedlich leben oder !? Aber ihr solltet auch nicht vergessen, dass Deutschland und USA sich beteiligt haben , diesen Genozid zu begehen!</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>['get√∂tet ', 'Aber ihr solltet auch nicht vergessen', 'Genozid ']</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>ekaterini@gmail.com</td>\n",
       "      <td>21</td>\n",
       "      <td>Zu wenig, auch noch ins Gef√§ngnis</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>['Zu wenig', 'ins Gef√§ngnis']</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user  instance_id  \\\n",
       "0     ekaterini@gmail.com         1237   \n",
       "1     ekaterini@gmail.com          210   \n",
       "2     ekaterini@gmail.com          494   \n",
       "3     ekaterini@gmail.com          123   \n",
       "4     ekaterini@gmail.com         1470   \n",
       "...                   ...          ...   \n",
       "1045  ekaterini@gmail.com         1443   \n",
       "1046  ekaterini@gmail.com            4   \n",
       "1047  ekaterini@gmail.com          645   \n",
       "1048  ekaterini@gmail.com         1142   \n",
       "1049  ekaterini@gmail.com           21   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             die pandemie mag zu ende sein. viele geimpfte stecken weiter in ihren impfsch√§den. üôÅ   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Wir sind hier nicht bei 'W√ºnsch dir was', sondern bei 'So isses' und das gilt auch f√ºr den schlechtesten Bundesminister f√ºr #Wirtschaft und #Klimaschutz Robert #Habeck \\nKeinen Bock mehr...was ist denn das f√ºr eine Aussage?   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Es kann wirklich niemand mehr behaupten, dass man nicht wusste, was man w√§hlt!   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Da ist der falsche bestraft wurden   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                 Vor 500 Jahren h√§tte man gesagt, die #AfD sei des Teufels Wiederg√§nger. Man h√§tte sie geteert und gefedert, auf einen Holzpfahl gespie√üt, mit Steinen beschwert und f√ºnf Meter tief vergraben. Nun leben wir in einer Demokratie und m√ºssen sie ertragen. Aber #LautgegenRechts d√ºrfen wir sein!   \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ...   \n",
       "1045                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Zwei wahre S√§tze f√ºr den Abend:\\nJedes einzelne zivile Opfer im Gazastreifen geht auf das Konto der Hamas. Keine relevante Armee der Welt handelt nach h√∂heren moralischen Ma√üst√§ben als die IDF.   \n",
       "1046                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Danke S√ºdafrikaüëè wenigstens schaut ihr nicht weg oder seid keine Marionette der USA!   \n",
       "1047  Ohne angeben zu wollen, habe ich #Putin schon vor 15 Jahren absolut alles zugetraut. Der Typ war schon immer total skrupellos & hinterliess nur Schutt & Asche. Nun l√§uft ihm die Zeit davon, seine Pl√§ne wie gewohnt umzusetzen und die #Demokratie r√ºckt n√§her an #Russland heran. Der offene √úberfall auf die #Ukraine ist klar ein Zeichen daf√ºr. Er wird immer brutaler & unberechenbarer werden. Wir sollten uns darauf einstellen & ihn stoppen, bevor der ganze Kontinent hineingezogen wird.\\n\\n#NATO #SupportUkraine #Baltikum #Estland #Lettland #Litauen #Georgien #Polen #Moldawien #Finnland\\n@Bundeskanzler\\n \\n@fdp\\n \\n@ABaerbock\\n \\n@ignaziocassis\\n \\n@Die_Gruenen\\n \\n@spdde\\n \\n@spdbt   \n",
       "1048                                                                                                                                                                                                                                                                                                                                                                                                                                                                Bitte Deutschland noch mehr Waffen liefern, bis alle Menschen da get√∂tet werden . Dann kann die Welt friedlich leben oder !? Aber ihr solltet auch nicht vergessen, dass Deutschland und USA sich beteiligt haben , diesen Genozid zu begehen!   \n",
       "1049                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Zu wenig, auch noch ins Gef√§ngnis   \n",
       "\n",
       "     sentiment       negative object_spans  \\\n",
       "0     negative                die pandemie   \n",
       "1     negative  ['Klimaschutz', 'Habeck ']   \n",
       "2      neutral                         ...   \n",
       "3     negative                         ...   \n",
       "4     negative                        #AfD   \n",
       "...        ...                         ...   \n",
       "1045  negative                       Hamas   \n",
       "1046  positive                         ...   \n",
       "1047  negative                      #Putin   \n",
       "1048  negative                         ...   \n",
       "1049  negative                         ...   \n",
       "\n",
       "                                                                                                                                                                               negative spans  \\\n",
       "0                                                                                                                                 ['viele geimpfte stecken weiter in ihren impfsch√§den', 'üôÅ']   \n",
       "1                                                                                                                                        ['schlechtesten Bundesminister', 'Keinen Bock mehr']   \n",
       "2                                                                                                                                                                                         ...   \n",
       "3                                                                                                                                                                 der falsche bestraft wurden   \n",
       "4     ['Man h√§tte sie geteert und gefedert, auf einen Holzpfahl gespie√üt, mit Steinen beschwert und f√ºnf Meter tief vergraben.', 'Nun leben wir in einer Demokratie und m√ºssen sie ertragen']   \n",
       "...                                                                                                                                                                                       ...   \n",
       "1045                                                                                                                                                             zivile Opfer im Gazastreifen   \n",
       "1046                                                                                                                                                                                      ...   \n",
       "1047                                                                    ['Der Typ war schon immer total skrupellos', 'hinterliess nur Schutt &amp; Asche.', 'brutaler &amp; unberechenbarer']   \n",
       "1048                                                                                                                        ['get√∂tet ', 'Aber ihr solltet auch nicht vergessen', 'Genozid ']   \n",
       "1049                                                                                                                                                            ['Zu wenig', 'ins Gef√§ngnis']   \n",
       "\n",
       "                                     positive spans positive object_spans  \n",
       "0                                               ...                   ...  \n",
       "1                                               ...                   ...  \n",
       "2                                               ...                   ...  \n",
       "3                                               ...                   ...  \n",
       "4                                               ...                   ...  \n",
       "...                                             ...                   ...  \n",
       "1045                                            ...                   ...  \n",
       "1046  ['Danke S√ºdafrikaüëè', 'seid keine Marionette']                   ...  \n",
       "1047                                            ...                   ...  \n",
       "1048                                            ...                   ...  \n",
       "1049                                            ...                   ...  \n",
       "\n",
       "[1050 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train_subset.csv')\n",
    "pd.options.display.max_colwidth = 1000\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('de_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "991"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignoring neutrals as we do not consider these for NER evaluation\n",
    "df = df[df['sentiment'] != 'neutral']\n",
    "df = df.reset_index(drop=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "991\n"
     ]
    }
   ],
   "source": [
    "#Creating lists of outputs as a dictionary of 'tokens and labels', instructions and inputs\n",
    "emojis = ['üëè','ü§î','ü§°','üíô','ü§ù','ü•≤','ü§©','‚úã','üòÖ','üíõ','üî•','üëç','üíî','ü§Æ','üíö','ü§¶','üí™','üò∑','‚úä','ü§¢','üò≠','üôå','üò£','ü•∫','üôÅ']\n",
    "inputs = []\n",
    "outputs = []\n",
    "instructions = []\n",
    "#instruction = 'Classify the sentiment of each word in the texts into ONE of the two classes: negative or positive and O if neutral. Split the answer in a dictionary with two keys:Tokens and Labels'\n",
    "instruction = 'Classify the sentiment of each word in the texts into ONE of the two classes: negative or positive. Split the answer in a dictionary with two keys:Tokens and Labels. Text:'\n",
    "for i in range(len(df)):\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    inputs.append(df['text'][i])\n",
    "    instructions.append(instruction)\n",
    "    tokenized_text=nlp(df['text'][i])\n",
    "    for token in tokenized_text:\n",
    "        token = str(token)\n",
    "        if token in df['negative object_spans'][i] or token in df['negative spans'][i] or token in df['positive object_spans'][i] or token in df['positive spans'][i]:\n",
    "            if token.isalpha() or token in emojis :\n",
    "                tokens.append(token)\n",
    "                labels.append(df['sentiment'][i])\n",
    "    outputs.append({'Tokens':tokens , 'Labels':labels})\n",
    "print(len(outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data_LLM = pd.DataFrame(columns=['output','input','instruction'])\n",
    "df_train_data_LLM['output'] = outputs\n",
    "df_train_data_LLM['input'] = inputs\n",
    "df_train_data_LLM['instruction'] = instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a csv file with generated outputs, inputs and instructions to use as training data\n",
    "df_train_data_LLM.to_csv('train_subset_llm_tokenClassification_latest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 991 examples [00:00, 29820.04 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 991/991 [00:00<00:00, 55850.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#Adding the EOS_TOKEN to the tokenized output!! Otherwise you'll get infinite generations!\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files={'train':'train_subset_llm_tokenClassification_latest.csv'})\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset['train'],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 10,\n",
    "        warmup_steps = 5,\n",
    "        #max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 500,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"fine-tuning-gemma-withUnsloth-tokenClassification\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA A100 80GB PCIe. Max memory = 79.138 GB.\n",
      "5.83 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 991 | Num Epochs = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 1,240\n",
      " \"-____-\"     Number of trainable parameters = 50,003,968\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1240' max='1240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1240/1240 39:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.707600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.069100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing the model to hugging face hub, for future inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"<huggingfaceuser>/fine-tuned-gemmaModel_tokenClassification_10epochs\") # Online saving\n",
    "tokenizer.push_to_hub(\"<huggingfaceuser>/fine-tuned-gemmaModel_tokenClassification_10epochs\") # Online saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
