{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install transformers\n",
    "%pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test_subset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text set for getting the results from LLM\n",
    "texts = list(df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat propmt used to get the labels of sequences using the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ltstorage/home/1samreen/miniconda3/envs/new_saint_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Gemma patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.138 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Unsloth 2024.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "load_in_4bit = True\n",
    "dtype = None\n",
    "if True:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"Hina541/fine-tuned-gemmaModelSequence_latest_20Epochs\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 512,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(model)\n",
    "prompt = f\"Classify the sentiment of each text into ONE of the three classes: neutral, negative or positive. Text:\"\n",
    "\n",
    "answers = []\n",
    "#text_list =[ \"Ukraine deckt auf üòÇüòÇüòÇüòÇüòÇ\\nDie mussten was sagen, nicht das man denkt der Staat w√§re korrupt und es w√ºrde keinen Kampf dagegen geben ‚òùÔ∏è\\n\\nKorrupt bleibt korrupt - auch die, da wird sich auch nichts √§nder üòÜ\",'Nur Politiker wollen Krieg ! Was wenn keiner mehr Bock hat f√ºr diese 5 Politiker zu sterben warum drehen wir den Spie√ü nich einfach um ? üòï frage f√ºr n Freund']\n",
    "for text in texts:\n",
    "    chat_template = [{\"role\" : \"user\", \"content\" : f\"{prompt} {text}\"}]\n",
    "\n",
    "    chat_prompt = tokenizer.apply_chat_template(chat_template, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer.encode(chat_prompt, add_special_tokens=False, return_tensors=\"pt\", max_length=512)\n",
    "    outputs = model.generate(input_ids=inputs.to(\"cuda\"), max_new_tokens = 512)\n",
    "    answer = tokenizer.decode(outputs[0])\n",
    "    answers.append(answer)\n",
    "    #print(answer)\n",
    "print(len(answers))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the results in the desired format , so as to evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for answer in answers:\n",
    "    #print(answer.split('<start_of_turn>model')[1].strip()[:-5])\n",
    "    labels.append(answer.split('<start_of_turn>model')[1].strip()[:-5])\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['predicted_sentiment_llm'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting only the classes 'Negative','Positive,'and 'Neutral' and ignoring other classes if any other are present\n",
    "df = df[('negative' == df['predicted_sentiment_llm']) |  ('positive' == df['predicted_sentiment_llm']) | ('neutral' == df['predicted_sentiment_llm'])]\n",
    "df = df.reset_index(drop=True)\n",
    "for i in range(len(df)):\n",
    "    df.loc[i,'predicted_sentiment_llm'] = str(df['predicted_sentiment_llm'][i]).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "for i in range(len(df)):\n",
    "    if df['sentiment'][i] == df['predicted_sentiment_llm'][i]:\n",
    "        correct_predictions += 1\n",
    "print('correct predictions:',correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of google gemma before fine tuning is 82.0 %\n"
     ]
    }
   ],
   "source": [
    "Accuracy = (correct_predictions/len(df))*100\n",
    "print('Accuracy of google gemma before fine tuning is',Accuracy, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ResultsAfterFineTuning_SequnceClassification_gemmaGoogle20Epochs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_saint_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
