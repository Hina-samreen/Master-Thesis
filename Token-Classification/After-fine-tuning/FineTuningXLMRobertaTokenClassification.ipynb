{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the CSV files that were previously generated for this purpose\n",
    "df_train = pd.read_csv('Pre-processed_dataForAutoModelTokenClassification-trainData_new.csv',converters={'tokens':literal_eval, 'ner_tags':literal_eval})\n",
    "df_validation = pd.read_csv('Pre-processed_dataForAutoModelTokenClassification-ValidationData_new.csv',converters={'tokens':literal_eval, 'ner_tags':literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>negative object_spans</th>\n",
       "      <th>negative spans</th>\n",
       "      <th>positive spans</th>\n",
       "      <th>positive object_spans</th>\n",
       "      <th>tokens</th>\n",
       "      <th>label</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>die pandemie mag zu ende sein. viele geimpfte ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>die pandemie</td>\n",
       "      <td>['viele geimpfte stecken weiter in ihren impfs...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[die, pandemie, mag, zu, ende, sein, viele, ge...</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Wir sind hier nicht bei 'WÃ¼nsch dir was', sond...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['Klimaschutz', 'Habeck ']</td>\n",
       "      <td>['schlechtesten Bundesminister', 'Keinen Bock ...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Wir, sind, hier, nicht, bei, WÃ¼nsch, dir, was...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Da ist der falsche bestraft wurden</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>der falsche bestraft wurden</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Da, ist, der, falsche, bestraft, wurden]</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Vor 500 Jahren hÃ¤tte man gesagt, die #AfD sei ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>#AfD</td>\n",
       "      <td>['Man hÃ¤tte sie geteert und gefedert, auf eine...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Vor, Jahren, hÃ¤tte, man, gesagt, die, AfD, se...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Die Ukrainer halten gerade ihren Kopf fÃ¼r uns ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>Ukrainer</td>\n",
       "      <td>[Die, Ukrainer, halten, gerade, ihren, Kopf, f...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>986</td>\n",
       "      <td>Zwei wahre SÃ¤tze fÃ¼r den Abend:\\nJedes einzeln...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Hamas</td>\n",
       "      <td>zivile Opfer im Gazastreifen</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Zwei, wahre, SÃ¤tze, fÃ¼r, den, Abend, Jedes, e...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>987</td>\n",
       "      <td>Danke SÃ¼dafrikağŸ‘ wenigstens schaut ihr nicht w...</td>\n",
       "      <td>positive</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>['Danke SÃ¼dafrikağŸ‘', 'seid keine Marionette']</td>\n",
       "      <td>...</td>\n",
       "      <td>[Danke, SÃ¼dafrika, ğŸ‘, wenigstens, schaut, ihr,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>988</td>\n",
       "      <td>Ohne angeben zu wollen, habe ich #Putin schon ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>#Putin</td>\n",
       "      <td>['Der Typ war schon immer total skrupellos', '...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Ohne, angeben, zu, wollen, habe, ich, Putin, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>989</td>\n",
       "      <td>Bitte Deutschland noch mehr Waffen liefern, bi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>['getÃ¶tet ', 'Aber ihr solltet auch nicht verg...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Bitte, Deutschland, noch, mehr, Waffen, liefe...</td>\n",
       "      <td>0</td>\n",
       "      <td>[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>990</td>\n",
       "      <td>Zu wenig, auch noch ins GefÃ¤ngnis</td>\n",
       "      <td>negative</td>\n",
       "      <td>...</td>\n",
       "      <td>['Zu wenig', 'ins GefÃ¤ngnis']</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Zu, wenig, auch, noch, ins, GefÃ¤ngnis]</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 2, 2, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>991 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text sentiment  \\\n",
       "0      0  die pandemie mag zu ende sein. viele geimpfte ...  negative   \n",
       "1      1  Wir sind hier nicht bei 'WÃ¼nsch dir was', sond...  negative   \n",
       "2      2                 Da ist der falsche bestraft wurden  negative   \n",
       "3      3  Vor 500 Jahren hÃ¤tte man gesagt, die #AfD sei ...  negative   \n",
       "4      4  Die Ukrainer halten gerade ihren Kopf fÃ¼r uns ...  positive   \n",
       "..   ...                                                ...       ...   \n",
       "986  986  Zwei wahre SÃ¤tze fÃ¼r den Abend:\\nJedes einzeln...  negative   \n",
       "987  987  Danke SÃ¼dafrikağŸ‘ wenigstens schaut ihr nicht w...  positive   \n",
       "988  988  Ohne angeben zu wollen, habe ich #Putin schon ...  negative   \n",
       "989  989  Bitte Deutschland noch mehr Waffen liefern, bi...  negative   \n",
       "990  990                  Zu wenig, auch noch ins GefÃ¤ngnis  negative   \n",
       "\n",
       "          negative object_spans  \\\n",
       "0                  die pandemie   \n",
       "1    ['Klimaschutz', 'Habeck ']   \n",
       "2                           ...   \n",
       "3                          #AfD   \n",
       "4                           ...   \n",
       "..                          ...   \n",
       "986                       Hamas   \n",
       "987                         ...   \n",
       "988                      #Putin   \n",
       "989                         ...   \n",
       "990                         ...   \n",
       "\n",
       "                                        negative spans  \\\n",
       "0    ['viele geimpfte stecken weiter in ihren impfs...   \n",
       "1    ['schlechtesten Bundesminister', 'Keinen Bock ...   \n",
       "2                          der falsche bestraft wurden   \n",
       "3    ['Man hÃ¤tte sie geteert und gefedert, auf eine...   \n",
       "4                                                  ...   \n",
       "..                                                 ...   \n",
       "986                       zivile Opfer im Gazastreifen   \n",
       "987                                                ...   \n",
       "988  ['Der Typ war schon immer total skrupellos', '...   \n",
       "989  ['getÃ¶tet ', 'Aber ihr solltet auch nicht verg...   \n",
       "990                      ['Zu wenig', 'ins GefÃ¤ngnis']   \n",
       "\n",
       "                                    positive spans positive object_spans  \\\n",
       "0                                              ...                   ...   \n",
       "1                                              ...                   ...   \n",
       "2                                              ...                   ...   \n",
       "3                                              ...                   ...   \n",
       "4                                              ...             Ukrainer    \n",
       "..                                             ...                   ...   \n",
       "986                                            ...                   ...   \n",
       "987  ['Danke SÃ¼dafrikağŸ‘', 'seid keine Marionette']                   ...   \n",
       "988                                            ...                   ...   \n",
       "989                                            ...                   ...   \n",
       "990                                            ...                   ...   \n",
       "\n",
       "                                                tokens  label  \\\n",
       "0    [die, pandemie, mag, zu, ende, sein, viele, ge...      0   \n",
       "1    [Wir, sind, hier, nicht, bei, WÃ¼nsch, dir, was...      0   \n",
       "2            [Da, ist, der, falsche, bestraft, wurden]      0   \n",
       "3    [Vor, Jahren, hÃ¤tte, man, gesagt, die, AfD, se...      0   \n",
       "4    [Die, Ukrainer, halten, gerade, ihren, Kopf, f...      1   \n",
       "..                                                 ...    ...   \n",
       "986  [Zwei, wahre, SÃ¤tze, fÃ¼r, den, Abend, Jedes, e...      0   \n",
       "987  [Danke, SÃ¼dafrika, ğŸ‘, wenigstens, schaut, ihr,...      1   \n",
       "988  [Ohne, angeben, zu, wollen, habe, ich, Putin, ...      0   \n",
       "989  [Bitte, Deutschland, noch, mehr, Waffen, liefe...      0   \n",
       "990            [Zu, wenig, auch, noch, ins, GefÃ¤ngnis]      0   \n",
       "\n",
       "                                              ner_tags  \n",
       "0           [0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1    [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, ...  \n",
       "2                                   [2, 2, 0, 0, 0, 0]  \n",
       "3    [2, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, ...  \n",
       "4    [2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, ...  \n",
       "..                                                 ...  \n",
       "986  [2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, ...  \n",
       "987         [1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2]  \n",
       "988  [2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 0, ...  \n",
       "989  [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, ...  \n",
       "990                                 [0, 0, 2, 2, 0, 0]  \n",
       "\n",
       "[991 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ltstorage/home/1samreen/miniconda3/envs/saint_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict,Dataset\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    'validation': Dataset.from_pandas(df_validation),\n",
    "    \"test\": Dataset.from_pandas(df_test)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'sentiment', 'negative object_spans', 'negative spans', 'positive spans', 'positive object_spans', 'tokens', 'label', 'ner_tags'],\n",
       "        num_rows: 991\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'text', 'sentiment', 'negative object_spans', 'negative spans', 'positive spans', 'positive object_spans', 'tokens', 'label', 'ner_tags'],\n",
       "        num_rows: 141\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'sentiment', 'negative object_spans', 'negative spans', 'positive spans', 'positive object_spans', 'tokens', 'label', 'ner_tags'],\n",
       "        num_rows: 283\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Classification - Fine tuning the model . Please refer to 'https://huggingface.co/docs/transformers/en/tasks/token_classification' for further information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ltstorage/home/1samreen/miniconda3/envs/saint_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-xlm-roberta-base-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/991 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 991/991 [00:00<00:00, 7781.65 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:00<00:00, 5503.98 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 283/283 [00:00<00:00, 7995.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True,remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 991\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 141\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 283\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive', 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = ['negative','positive','O']\n",
    "label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"negative\",1: \"positive\", 2:\"O\"}\n",
    "label2id = {\"negative\":0, \"positive\":1 , \"O\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"cardiffnlp/twitter-xlm-roberta-base-sentiment\", num_labels=3, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='465' max='465' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [465/465 04:50, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.731874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.797600</td>\n",
       "      <td>0.685727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.797600</td>\n",
       "      <td>0.686352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.669900</td>\n",
       "      <td>0.741035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.581200</td>\n",
       "      <td>0.716866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.581200</td>\n",
       "      <td>0.736067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.754967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.517200</td>\n",
       "      <td>0.769864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.790813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.429200</td>\n",
       "      <td>0.798384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.429200</td>\n",
       "      <td>0.811886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.827121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.828829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.378900</td>\n",
       "      <td>0.828316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.366600</td>\n",
       "      <td>0.829364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=465, training_loss=0.5075213719439763, metrics={'train_runtime': 291.5003, 'train_samples_per_second': 50.995, 'train_steps_per_second': 1.595, 'total_flos': 837957185025534.0, 'train_loss': 0.5075213719439763, 'epoch': 15.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fineTuningXLMRoberta-TokenClassification-Spacy\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_steps= 50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pushing the modelt o hugging face hub for use in future\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pushing the tokenizer to the same repository as the model\n",
    "tokenizer.push_to_hub('<huggingfaceuser>/fineTuningXLMRoberta-TokenClassification-Spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data using the above fine tuned model for NER Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Hina541/fineTuningXLMRoberta-TokenClassification-Spacy\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hina541/fineTuningXLMRoberta-TokenClassification-Spacy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "df_test = pd.read_csv('Pre-processed_dataForAutoModelTokenClassification-TestData_new.csv',converters={'tokens':literal_eval, 'ner_tags':literal_eval, 'ner_tags_for_NEREval':literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting label for each word using the fine tuned model\n",
    "def Extract_label_for_each_word(text):\n",
    "    tokens=[]\n",
    "    encoded = tokenizer(text,padding=True,truncation=True,return_tensors=\"pt\")\n",
    "    word_ids = encoded.word_ids()\n",
    "    with torch.no_grad():\n",
    "        logits =model(**encoded).logits\n",
    "    input_ids = encoded[\"input_ids\"].tolist()[0]\n",
    "    labels_appended =[]  \n",
    "    for word_id in sorted(set(word_ids) - {None}):\n",
    "        token_ids = [input_ids[i] for i in range(len(input_ids)) if word_ids[i]==word_id]\n",
    "        word = tokenizer.decode(token_ids,clean_up_tokenization_spaces=True).strip()\n",
    "        labels = [model.config.id2label[torch.argmax(logits[0][i]).item()] for i in range(len(input_ids)) if word_ids[i]==word_id]\n",
    "        tokens.append(word)\n",
    "        labels_appended.append(labels[0])\n",
    "    return tokens,labels_appended\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['words','labels_for_words']]=df_test[\"text\"].apply(lambda x : pd.Series(Extract_label_for_each_word(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if the words are same as original spans and adding it to a new dataframe as 'tokens', 'actual sentiment' and 'predicted sentiment' for NER evaluation\n",
    "tokens = []\n",
    "actual_sentiment = []\n",
    "predicted_sentiment =[]\n",
    "emojis = ['ğŸ™','ğŸ˜•','ğŸ‡ºğŸ‡¦ğŸ¤¡ğŸ‡©ğŸ‡ªğŸ¤£ğŸ¤£ğŸ¤£ğŸ¤£ğŸ†','ğŸ’™ğŸ’™ğŸ’™ğŸ‡©ğŸ‡ªğŸ‡©ğŸ‡ªğŸ‡©ğŸ‡ªğŸ‡©ğŸ‡ªğŸ’™ğŸ’™ğŸ’™','ğŸ˜¡ğŸ˜¡ğŸ˜¡','ğŸ¤“','ğŸ˜ƒ','ğŸ’™ğŸ’™ğŸ’™ğŸ’™','ğŸ˜†','ğŸ˜','ğŸ˜‰ğŸ˜‰','ğŸ™‚','ğŸ¤¡','âœ‹','ğŸ€â¤ï¸','ğŸ™ˆ','ğŸ˜','ğŸ¤','ğŸ¤¡ğŸ¤¡','ğŸ•Š','â¬†ï¸','ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚','ğŸ¤£ğŸ¤¦ğŸ¼','ğŸ’”','ğŸ‘ŒğŸ»','ğŸ‡µğŸ‡¸','ğŸ”¥','ğŸ˜¤','ğŸ™ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘','ğŸ¤·ğŸ»','â›”ï¸','ğŸ˜”','ğŸ‡®ğŸ‡±ğŸ’ª','ğŸ˜…ğŸ™ƒ','ğŸ‡®ğŸ‡±ğŸ‡®ğŸ‡±ğŸ‡®ğŸ‡±','ğŸ˜ªğŸ˜ªğŸ˜ªğŸ˜ªğŸ‰ğŸ‰ğŸ‰','ğŸ‡©ğŸ‡ªğŸ‡¨ğŸ‡­ğŸ‡¦ğŸ‡¹ğŸ‡¹ğŸ‡·','ğŸ‰','ğŸ’','ğŸ¤¡ğŸ¤¦','ğŸ˜‚ğŸ¤£ğŸ˜…','ğŸ¤ª','ğŸ‘ğŸ‘','ğŸ’¸','ğŸ˜…','ğŸ¤£ğŸ¤£.','âœŒğŸ½','ğŸ˜','!!!ğŸ‘ğŸ‘ğŸ‘ğŸ‘','ğŸ¤ğŸ¼','ğŸ¤¡ğŸ¤¡ğŸ¤¡ğŸ¤¡ğŸ¤¡ğŸ¤¡ğŸ¤¡ğŸ¤¡','ğŸ‡©ğŸ‡ªğŸ¤ğŸ»ğŸ‡ºğŸ‡¦','ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚','ğŸ‡©ğŸ‡ªğŸ’™ğŸ‡ºğŸ‡¦ğŸ’›ğŸ‡ªğŸ‡º','ğŸ˜³','ğŸ‘','ğŸŒ³ğŸŒ³ğŸŒ³','ğŸ‘ğŸ‘ğŸ’™ğŸ’™ğŸ’™ğŸ’™ğŸ’ªğŸ‡©ğŸ‡ª','ğŸ’™ğŸ‡®ğŸ‡±','ğŸ˜œ','ğŸ¤£','ğŸ˜Šâ˜ï¸','ğŸ‡¦ğŸ‡¹â¤ï¸ğŸ‡®ğŸ‡±','ğŸ™„ğŸ™„','ğŸ˜¬','ğŸ™„','ğŸŒ±','ğŸ˜¢ğŸ˜¢','ğŸ‘ğŸ’›','ğŸ˜‚ğŸ˜‚','ğŸ‡©ğŸ‡ªğŸ¤¡','ğŸ¤·','ğŸ’ª','ğŸ‘','ğŸ˜±','ğŸ¤¦ğŸ»','ğŸ¤¦ğŸ¼','ğŸ‡µğŸ‡¸','ğŸ˜•','ğŸ˜','ğŸ˜¥ğŸ™','ğŸ˜‚','ğŸ˜‚.','ğŸ˜Š','ğŸ˜ ','ğŸ˜®','ğŸ’«','âœŠğŸ¼','ğŸ‘ğŸ¼','ğŸ¤”','ğŸ˜ƒ','ğŸŒğŸ€','ğŸ˜‰','ğŸŒ»','ğŸ‡ºğŸ‡¦','ğŸ˜¡ğŸ˜¡','âœŒğŸ¼','ğŸ•Šï¸','ğŸ˜‘','ğŸ˜†ğŸ˜†ğŸ˜†','ğŸ¤¨','ğŸ‡©ğŸ‡ª','ğŸ¤¦','â™‚ï¸ğŸ¤¦','ğŸ˜¢','âŒ','ğŸ’ªğŸ’ªğŸ’ª','ğŸ‡ºğŸ‡¸','ğŸ‡®ğŸ‡·','ğŸ³ï¸','ğŸŒˆ', 'âœŒï¸ğŸ‡µğŸ‡¸â™¥ï¸ğŸ‡§ğŸ‡·â™¥ï¸ğŸ‡¿ğŸ‡¦âœŒï¸', 'â™‚ï¸ğŸ¤¡','ğŸ‡µğŸ‡¸ğŸ’”ğŸ’”ğŸ’”','ğŸ›‘','ğŸš«!!!','ğŸ‘ğŸ˜‚','ğŸ’š','ğŸ™‚.','âŒâŒğŸ’”','ğŸ™ˆğŸ™ˆğŸ™ˆ','ğŸ¤¦','ğŸ‡©ğŸ‡ªğŸ‡®ğŸ‡±ğŸ™','ğŸ™','ğŸ™‡ğŸ¼','ğŸŒâ™»ï¸ğŸ”¥ğŸ’¸ğŸ’¥ğŸ¤·','ğŸ‘!!','ğŸ˜‚ğŸ¤”ğŸ˜·','ğŸ’šğŸ”¥','ğŸ’šğŸŒ³','ğŸ˜”ğŸ˜”','ğŸ˜¡','ğŸ¥°','ğŸ˜‚ğŸ˜‚ğŸ˜‚','ğŸ¤¡ğŸ¤¡ğŸ¤¡','ğŸ‡®ğŸ‡±','ğŸ™ğŸ™ğŸ˜”ğŸ˜”','ğŸ™Œ','ğŸ˜£','ğŸ¤©','ğŸ‘ğŸ‘ğŸ‘ğŸ‘']\n",
    "df_tokens_results = pd.DataFrame(columns=['tokens','actual_sentiment','predicted_sentiment'])\n",
    "for i in range(0,len(df_test)):\n",
    "    for index,token in enumerate(df_test['words'][i]):\n",
    "        if token not in emojis:\n",
    "            item = ''.join(e for e in token if e.isalnum())\n",
    "        else:\n",
    "            item = token\n",
    "        if item != '':\n",
    "            if item in df_test['negative object_spans'][i] or item in df_test['negative spans'][i] or item in df_test['positive object_spans'][i] or item in df_test['positive spans'][i] or item in emojis:\n",
    "                #if item.strip().isalpha() or item.strip() in emojis:\n",
    "                tokens.append(item)\n",
    "                actual_sentiment.append(df_test['sentiment'][i])\n",
    "                predicted_sentiment.append(df_test['labels_for_words'][i][index])\n",
    "            else:\n",
    "                tokens.append(item)\n",
    "                actual_sentiment.append('O')\n",
    "                predicted_sentiment.append('O')\n",
    "    tokens.append(' ')\n",
    "    actual_sentiment.append(' ')\n",
    "    predicted_sentiment.append(' ')\n",
    "df_tokens_results['tokens'] = tokens\n",
    "df_tokens_results['actual_sentiment'] = actual_sentiment\n",
    "df_tokens_results['predicted_sentiment'] = predicted_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the results to a csv for further ner evaluation\n",
    "df_tokens_results.to_csv('ProperTokenClassification_Spacy2ndattempt.csv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the results of NER Evaluation\n",
    "#### python conlleval_perl.py -r < ProperTokenClassification_Spacy2ndattempt.csv -- command used for NER evaluation\n",
    "#### processed 7556 tokens with 2827 phrases; found: 1687 phrases; correct: 1594.\n",
    "#### accuracy:  83.68%; precision:  94.49%; recall:  56.38%; FB1:  70.62\n",
    "         #### negative: precision:  95.37%; recall:  59.15%; FB1:  73.02  1511\n",
    "         #### positive: precision:  86.93%; recall:  39.13%; FB1:  53.97  176"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saint_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
